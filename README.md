  # 🧠 Hey there, I’m Mauricio V

Welcome to my GitHub.

I’m building a **modular AI memory system** that runs entirely offline — using multiple LLMs across my own hardware, with full routing, memory persistence, and GPU-aware orchestration.

### 🔗 [Try My Custom GPT Assistant](https://chatgpt.com/g/g-686d56d1a8048191bd32fdb5704d2eb4-memoryarchitect-gpt)  
This GPT helps coordinate tasks across my models. It exists because I couldn’t manage it all myself — so I built something that could.
 
 ---

## 🧩 Core Ideas

- Chain multiple local models together (Capybara → Hermes → MythoMax)
- Process and reflect on memory logs — emotion, tone, summaries
- GPU-aware task delegation across multiple machines
- Privacy-first: no cloud APIs, everything runs locally
- VPN-secured, MFA-protected orchestration (in progress)

## 🖥️ Powered by:
`llama.cpp`, `text-generation-webui`, `Ollama`, `FastAPI`, `Gradio`, and way too much YAML

## 🔭 What’s Next:
- Web dashboard with live GPU routing
- Docker containers per node
- Voice-to-memory pipelines (Whisper + Ollama)

## 🪪 [LinkedIn](https://www.linkedin.com/in/mauricio-ventura-52a14425a/) 

Thanks for stopping by ✌️
