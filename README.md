  # Hey there, I’m Mauricio V

Welcome to my GitHub.

Currenlty my focus Is on building a **modular AI memory system** that runs entirely offline — using multiple LLMs across my own hardware, with full routing, memory persistence, and GPU and RAM management.
 
 ## 🧩 Ideas/Projects Completed and Upcoming  

- Chained multiple local models together (Capybara → Hermes → MythoMax)
- Used Python and JSON files to process and reflect on memory logs — emotion, tone, summaries (parser, watcher)
- GPU-aware task delegation across multiple machines
- Privacy-first: no cloud APIs, everything runs locally
- Sky Watcher, uses data from api.nasa.gov (comets, meteors, etc) and logs fed into custom LLM and saved(In Progress)
- Local App that transcribes voice or other parties, while being fed to LLM, then being processsed and showing real-time suggesttions

### 🔗 [Try My Custom GPT Assistant](https://chatgpt.com/g/g-686d56d1a8048191bd32fdb5704d2eb4-memoryarchitect-gpt) 🤖
This GPT helps coordinate tasks across my models and helps with ways to further opitmize the flow and stablity of the architecture. It exists because I couldn’t manage it all myself — so I built something that could take some of the load.

## 🖥️ Powered by:
`llama.cpp`, `text-generation-webui`, `Ollama`, `FastAPI`, `Gradio`, and way too many Python & JSON files...

## 🔭 What’s Next:
- Web dashboard with live GPU routing
- Docker containers per node
- Voice-to-memory pipelines (Whisper + Ollama)
- VPN-secured, MFA-protected orchestration 

## 🪪 [LinkedIn](https://www.linkedin.com/in/mauricio-ventura-52a14425a/) 

Thanks for stopping by ✌️
