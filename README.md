  # ğŸ§  Hey there, Iâ€™m Mauricio V

Welcome to my GitHub.

Iâ€™m building a **modular AI memory system** that runs entirely offline â€” using multiple LLMs across my own hardware, with full routing, memory persistence, and GPU-aware orchestration.

### ğŸ”— [Try My Custom GPT Assistant](https://chatgpt.com/g/g-686d56d1a8048191bd32fdb5704d2eb4-memoryarchitect-gpt)  
This GPT helps coordinate tasks across my models. It exists because I couldnâ€™t manage it all myself â€” so I built something that could.
 
 ---

## ğŸ§© Core Ideas

- Chain multiple local models together (Capybara â†’ Hermes â†’ MythoMax)
- Process and reflect on memory logs â€” emotion, tone, summaries
- GPU-aware task delegation across multiple machines
- Privacy-first: no cloud APIs, everything runs locally
- VPN-secured, MFA-protected orchestration (in progress)

## ğŸ–¥ï¸ Powered by:
`llama.cpp`, `text-generation-webui`, `Ollama`, `FastAPI`, `Gradio`, and way too much YAML

## ğŸ”­ Whatâ€™s Next:
- Web dashboard with live GPU routing
- Docker containers per node
- Voice-to-memory pipelines (Whisper + Ollama)

## ğŸªª [LinkedIn](https://www.linkedin.com/in/mauricio-ventura-52a14425a/) 

Thanks for stopping by âœŒï¸
