  # Hey there, Iâ€™m Mauricio V

Welcome to my GitHub.

Currenlty my focus Is on building a **modular AI memory system** that runs entirely offline â€” using multiple LLMs across my own hardware, with full routing, memory persistence, and GPU and RAM management.
 
 ## ğŸ§© Ideas/Projects Completed and Upcoming  

- Chained multiple local models together (Capybara â†’ Hermes â†’ MythoMax)
- Used Python and JSON files to process and reflect on memory logs â€” emotion, tone, summaries (parser, watcher)
- GPU-aware task delegation across multiple machines
- Privacy-first: no cloud APIs, everything runs locally
- Sky Watcher, uses data from api.nasa.gov (comets, meteors, etc) and logs fed into custom LLM and saved(In Progress)
- Local App that transcribes voice or other parties, while being fed to LLM, then being processsed and showing real-time suggesttions

### ğŸ”— [Try My Custom GPT Assistant](https://chatgpt.com/g/g-686d56d1a8048191bd32fdb5704d2eb4-memoryarchitect-gpt) ğŸ¤–
This GPT helps coordinate tasks across my models and helps with ways to further opitmize the flow and stablity of the architecture. It exists because I couldnâ€™t manage it all myself â€” so I built something that could take some of the load.

## ğŸ–¥ï¸ Powered by:
`llama.cpp`, `text-generation-webui`, `Ollama`, `FastAPI`, `Gradio`, and way too many Python & JSON files...

## ğŸ”­ Whatâ€™s Next:
- Web dashboard with live GPU routing
- Docker containers per node
- Voice-to-memory pipelines (Whisper + Ollama)
- VPN-secured, MFA-protected orchestration 

## ğŸªª [LinkedIn](https://www.linkedin.com/in/mauricio-ventura-52a14425a/) 

Thanks for stopping by âœŒï¸
